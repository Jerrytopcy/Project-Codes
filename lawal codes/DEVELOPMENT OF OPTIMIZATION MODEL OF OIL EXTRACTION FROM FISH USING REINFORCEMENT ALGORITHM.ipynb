{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b683dba-ec5c-4342-9f6a-45750d4856f2",
   "metadata": {},
   "source": [
    "This command installs the necessary libraries *(numpy, pandas, scikit-learn, and matplotlib)*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fa5e3-ec27-43dc-8504-d5fa747ab11c",
   "metadata": {},
   "source": [
    "### Step 1: Set Up the Environment\n",
    "First, ensure you have all the required libraries installed. You can do this by running the following command in your terminal or directly in a Jupyter Notebook cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63017a47-4362-44d4-904e-d9e0fc03c244",
   "metadata": {},
   "source": [
    "### Step 2: Generate Synthetic Data\n",
    "Next, generate the synthetic dataset that will simulate the oil extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c5f012e-139e-4c5c-8e6e-9f83f2499f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Time</th>\n",
       "      <th>Oil_Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.727006</td>\n",
       "      <td>6.851329</td>\n",
       "      <td>53.553512</td>\n",
       "      <td>61.210302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.535715</td>\n",
       "      <td>10.419009</td>\n",
       "      <td>52.228092</td>\n",
       "      <td>75.856523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86.599697</td>\n",
       "      <td>13.729458</td>\n",
       "      <td>111.562912</td>\n",
       "      <td>91.170136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.932924</td>\n",
       "      <td>12.322249</td>\n",
       "      <td>52.459158</td>\n",
       "      <td>58.732230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.800932</td>\n",
       "      <td>13.065611</td>\n",
       "      <td>54.475475</td>\n",
       "      <td>61.755318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.799726</td>\n",
       "      <td>11.587834</td>\n",
       "      <td>98.345844</td>\n",
       "      <td>68.980136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52.904181</td>\n",
       "      <td>11.922766</td>\n",
       "      <td>70.476586</td>\n",
       "      <td>55.286893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.308807</td>\n",
       "      <td>13.491957</td>\n",
       "      <td>99.903950</td>\n",
       "      <td>88.136096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80.055751</td>\n",
       "      <td>7.496680</td>\n",
       "      <td>35.882954</td>\n",
       "      <td>47.908929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85.403629</td>\n",
       "      <td>9.894250</td>\n",
       "      <td>73.881407</td>\n",
       "      <td>74.824900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51.029225</td>\n",
       "      <td>7.212094</td>\n",
       "      <td>33.025224</td>\n",
       "      <td>36.044998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Temperature   Pressure        Time  Oil_Yield\n",
       "0     68.727006   6.851329   53.553512  61.210302\n",
       "1     97.535715  10.419009   52.228092  75.856523\n",
       "2     86.599697  13.729458  111.562912  91.170136\n",
       "3     79.932924  12.322249   52.459158  58.732230\n",
       "4     57.800932  13.065611   54.475475  61.755318\n",
       "5     57.799726  11.587834   98.345844  68.980136\n",
       "6     52.904181  11.922766   70.476586  55.286893\n",
       "7     93.308807  13.491957   99.903950  88.136096\n",
       "8     80.055751   7.496680   35.882954  47.908929\n",
       "9     85.403629   9.894250   73.881407  74.824900\n",
       "10    51.029225   7.212094   33.025224  36.044998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic features\n",
    "temperature = np.random.uniform(50, 100, n_samples)  # Temperature in degrees Celsius\n",
    "pressure = np.random.uniform(5, 15, n_samples)       # Pressure in MPa\n",
    "time = np.random.uniform(30, 120, n_samples)         # Time in minutes\n",
    "\n",
    "# Simulate oil yield based on the features with some added noise\n",
    "oil_yield = (0.5 * temperature) + (0.8 * pressure) + (0.3 * time) + np.random.normal(0, 5, n_samples)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Temperature': temperature,\n",
    "    'Pressure': pressure,\n",
    "    'Time': time,\n",
    "    'Oil_Yield': oil_yield\n",
    "})\n",
    "\n",
    "# Display the first few rows of the synthetic data\n",
    "data.head(11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd323418-4c45-44c3-80cd-300f0f623f46",
   "metadata": {},
   "source": [
    "#### What this does:\n",
    "\n",
    "Generates random values for temperature, pressure, and time.\n",
    "Computes a synthetic oil yield with some added noise to make the data more realistic.\n",
    "Stores the generated data in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991d4fd-efa1-4fa5-ac65-2e37adc717a8",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess the Data\n",
    "Normalize the dataset to ensure all features are on a similar scale, which is important for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48c88fa2-d778-4eba-b3ce-21cce657f47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Time</th>\n",
       "      <th>Oil_Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.371735</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.262269</td>\n",
       "      <td>0.412167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950755</td>\n",
       "      <td>0.540740</td>\n",
       "      <td>0.247509</td>\n",
       "      <td>0.633126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730954</td>\n",
       "      <td>0.873049</td>\n",
       "      <td>0.908233</td>\n",
       "      <td>0.864155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.596960</td>\n",
       "      <td>0.731791</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.374781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.152134</td>\n",
       "      <td>0.806411</td>\n",
       "      <td>0.272535</td>\n",
       "      <td>0.420389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.152110</td>\n",
       "      <td>0.658069</td>\n",
       "      <td>0.761054</td>\n",
       "      <td>0.529386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.053716</td>\n",
       "      <td>0.691690</td>\n",
       "      <td>0.450716</td>\n",
       "      <td>0.322803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.865799</td>\n",
       "      <td>0.849208</td>\n",
       "      <td>0.778404</td>\n",
       "      <td>0.818382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.599429</td>\n",
       "      <td>0.247391</td>\n",
       "      <td>0.065498</td>\n",
       "      <td>0.211496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.706915</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.488630</td>\n",
       "      <td>0.617563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.016031</td>\n",
       "      <td>0.218824</td>\n",
       "      <td>0.033676</td>\n",
       "      <td>0.032511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Temperature  Pressure      Time  Oil_Yield\n",
       "0      0.371735  0.182609  0.262269   0.412167\n",
       "1      0.950755  0.540740  0.247509   0.633126\n",
       "2      0.730954  0.873049  0.908233   0.864155\n",
       "3      0.596960  0.731791  0.250082   0.374781\n",
       "4      0.152134  0.806411  0.272535   0.420389\n",
       "5      0.152110  0.658069  0.761054   0.529386\n",
       "6      0.053716  0.691690  0.450716   0.322803\n",
       "7      0.865799  0.849208  0.778404   0.818382\n",
       "8      0.599429  0.247391  0.065498   0.211496\n",
       "9      0.706915  0.488064  0.488630   0.617563\n",
       "10     0.016031  0.218824  0.033676   0.032511"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "scaled_data.head(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efaf792-8ff0-4c63-8375-d810872fe043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79ae58e8-5aa3-4549-a3e0-06a636a805d8",
   "metadata": {},
   "source": [
    "#### What this does:\n",
    "\n",
    "Scales all the features in the dataset to a range between 0 and 1 using MinMaxScaler.\n",
    "Displays the first few rows of the scaled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d8f94-322b-46ce-b13b-3fb527358dc6",
   "metadata": {},
   "source": [
    "### Step 4: Design the Reinforcement Learning Model\n",
    "Now, we’ll define the action space and a simple reward function. This function will simulate the impact of actions on the oil yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7a68f97-cf42-4bb1-95c6-5583b3d5d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the action space (adjustments in temperature, pressure, and time)\n",
    "actions = ['increase_temp', 'decrease_temp', 'increase_pressure', 'decrease_pressure', 'increase_time', 'decrease_time']\n",
    "\n",
    "#  Reward function\n",
    "def reward_function(current_state, action):\n",
    "    \"\"\"\n",
    "    Simulate the effect of an action on the oil yield.\n",
    "    \"\"\"\n",
    "    temperature, pressure, time = current_state\n",
    "\n",
    "    # Adjust state based on action\n",
    "    if action == 'increase_temp':\n",
    "        temperature += 2\n",
    "    elif action == 'decrease_temp':\n",
    "        temperature -= 2\n",
    "    elif action == 'increase_pressure':\n",
    "        pressure += 1\n",
    "    elif action == 'decrease_pressure':\n",
    "        pressure -= 1\n",
    "    elif action == 'increase_time':\n",
    "        time += 5\n",
    "    elif action == 'decrease_time':\n",
    "        time -= 5\n",
    "\n",
    "    # Calculate the new oil yield\n",
    "    oil_yield = (0.5 * temperature) + (0.8 * pressure) + (0.3 * time) + np.random.normal(0, 5)\n",
    "\n",
    "    # Reward is proportional to oil yield (simplified)\n",
    "    reward = oil_yield\n",
    "\n",
    "    return np.array([temperature, pressure, time]), reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef535f7-2734-4d88-b6c8-8960ba3b40bb",
   "metadata": {},
   "source": [
    "#### What this does:\n",
    "\n",
    "Defines a list of possible actions the reinforcement learning agent can take.\n",
    "Provides a reward_function that calculates the new state and reward after an action is taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975eac07-4d79-47d7-a4a6-a0427fa62d22",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model\n",
    "Implement a basic Q-learning algorithm and train it using the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1599f1c1-423d-401a-84cf-efc28cc59a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-table\n",
    "q_table = np.zeros((1000, len(actions)))  # 1000 can accommodate the largest index (999)\n",
    "\n",
    "# Discretize the state space and flatten it into a single index\n",
    "def discretize_state(state):\n",
    "    discrete_state = (state * 0.1).astype(int)  # Adjusted to fit within [0,9]\n",
    "    return discrete_state[0] * 100 + discrete_state[1] * 10 + discrete_state[2]\n",
    "\n",
    "# Training loop remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f26edb9b-64d0-45c2-a9d3-db720c91efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-table with appropriate dimensions\n",
    "state_dimensions = [10, 10, 10]  # Discretized state has 10 possible values per dimension\n",
    "q_table = np.zeros((*state_dimensions, len(actions)))  # Shape: (10, 10, 10, len(actions))\n",
    "\n",
    "# Discretize the state space\n",
    "def discretize_state(state):\n",
    "    return tuple((state * 0.1).astype(int))  # Adjusted to fit within [0,9]\n",
    "\n",
    "# Training loop remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3d797f0-ab53-4d14-86ab-7ab51a2bcd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 complete.\n",
      "Episode 10 complete.\n",
      "Episode 20 complete.\n",
      "Episode 30 complete.\n",
      "Episode 40 complete.\n",
      "Episode 50 complete.\n",
      "Episode 60 complete.\n",
      "Episode 70 complete.\n",
      "Episode 80 complete.\n",
      "Episode 90 complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the action space\n",
    "actions = ['increase_temp', 'decrease_temp', 'increase_pressure', 'decrease_pressure', 'increase_time', 'decrease_time']\n",
    "\n",
    "# Initialize the Q-table\n",
    "state_size = 10  # Number of possible values after discretization per feature\n",
    "q_table = np.zeros((state_size ** 3, len(actions)))  # 3 features, hence state_size^3\n",
    "\n",
    "# Set learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Discretize the state space for Q-learning\n",
    "def discretize_state(state, state_min, state_max):\n",
    "    \"\"\"\n",
    "    Normalize and discretize the continuous state into a single index.\n",
    "    \"\"\"\n",
    "    normalized_state = (state - state_min) / (state_max - state_min)\n",
    "    discrete_state = (normalized_state * (state_size - 1)).astype(int)\n",
    "    discrete_state = np.clip(discrete_state, 0, state_size - 1)\n",
    "    return np.ravel_multi_index(discrete_state, (state_size, state_size, state_size))\n",
    "\n",
    "# Define the state ranges\n",
    "state_min = np.array([50, 5, 30])\n",
    "state_max = np.array([100, 15, 120])\n",
    "\n",
    "# Simplified reward function\n",
    "def reward_function(state, action):\n",
    "    # Simpler state update and reward calculation\n",
    "    new_state = state + np.random.randint(-2, 3, size=state.shape)  # Smaller random change\n",
    "    reward = -np.sum(np.abs(new_state - np.array([75, 10, 75])))  # Reward is higher for states closer to (75, 10, 75)\n",
    "    return new_state, reward\n",
    "\n",
    "# Training loop with reduced iterations\n",
    "for episode in range(100):  # Reduced from 1000 to 100 episodes\n",
    "    state = np.array([50, 10, 60])  # Start from a baseline state\n",
    "    for _ in range(10):  # Reduced from 100 to 10 steps per episode\n",
    "        state_discrete = discretize_state(state, state_min, state_max)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_idx = random.randint(0, len(actions) - 1)  # Explore\n",
    "        else:\n",
    "            action_idx = np.argmax(q_table[state_discrete])  # Exploit\n",
    "\n",
    "        action = actions[action_idx]\n",
    "        new_state, reward = reward_function(state, action)\n",
    "        new_state_discrete = discretize_state(new_state, state_min, state_max)\n",
    "\n",
    "        # Q-learning update rule\n",
    "        q_table[state_discrete][action_idx] = (1 - alpha) * q_table[state_discrete][action_idx] + alpha * (reward + gamma * np.max(q_table[new_state_discrete]))\n",
    "\n",
    "        state = new_state  # Move to the new state\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f'Episode {episode} complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf1251-dd6a-4a8c-95cb-552f89047ef3",
   "metadata": {},
   "source": [
    "#### What this does:\n",
    "\n",
    "Initializes a Q-table and sets learning parameters.\n",
    "Trains the model using Q-learning, with episodes where the agent takes actions and updates the Q-table based on the rewards received."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7463f89-2873-4c79-b897-0027c3162ad4",
   "metadata": {},
   "source": [
    "### Step 6: Test the Model\n",
    "After training, test the model to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073fcec6-36c7-4204-8f1d-393a6a6fbe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward during testing: -212\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained model\n",
    "state = np.array([55, 12, 75])  # Start from a new initial state\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(10):  # Limit the number of steps to match the training setup\n",
    "    state_discrete = discretize_state(state, state_min, state_max)  # Ensure state discretization matches training\n",
    "    action_idx = np.argmax(q_table[state_discrete])  # Always exploit during testing\n",
    "    action = actions[action_idx]\n",
    "    new_state, reward = reward_function(state, action)\n",
    "    total_reward += reward\n",
    "    state = new_state\n",
    "\n",
    "print(f\"Total reward during testing: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227a8d8-7ca3-4549-8ea1-b1417a2502d3",
   "metadata": {},
   "source": [
    "#### What this does:\n",
    "\n",
    "Tests the model by starting from a new initial state and seeing how well it optimizes the oil extraction process.\n",
    "Outputs the total reward, which reflects how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814ed42-8511-4eea-b377-23ee9d82bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
